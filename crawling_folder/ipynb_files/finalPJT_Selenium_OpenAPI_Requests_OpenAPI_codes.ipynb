{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44bc4496",
   "metadata": {},
   "source": [
    "# 엑셀 파일 말고 사이트별 category id CSV 파일"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f6311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./NS_main_mid_sub1_cat.csv')\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b653441",
   "metadata": {},
   "source": [
    "# selenium 크롤링 -> 사용안함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0652327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent(verify_ssl=False) # inspried by Nara's code\n",
    "fake_ua = ua.random\n",
    "headers = { 'user-agent' : fake_ua }\n",
    "\n",
    "url = 'https://search.shopping.naver.com/search/category/{}'.format('50003086')\n",
    "res = requests.get(url, headers=headers)\n",
    "\n",
    "soup = BeautifulSoup(res.text, 'html.parser')\n",
    "# soup.select('.style_content__xWg5l .noResult_no_result__6kgzY')\n",
    "\n",
    "mid_cat_list = soup.select('.filter_finder_list__4PE1C .filter_text_over__iesoO')\n",
    "mid_cat_list\n",
    "\n",
    "# mid_cat_list = [mid_cat.text for mid_cat in mid_cat_list] # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674ae52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 눌렀는데 없는 거면 pass\n",
    "# 1. 눌렀는데 있는 건데 동일하면 pass\n",
    "# 2. 동일하지 않으면 긁어옴\n",
    "# 3. 여기서 만약에 들어갔는데 리스트로로 또 뽑았는데 없으면 => 검색결과가 없습니다\n",
    "#        있으면 검색결과가 있는 것이라고 확인해주는 건 있으면 좋을 것 같음. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c852c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a72d290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why this process?\n",
    "# => 하려고하는 메인 카테고리를 정하고 그에 따른 카테고리 이름, 아이디를 추출하는 데이터 프레임을 뽑기 위해\n",
    "# => 스마트 스토어 접근이 어려웠다면? \n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from selenium import webdriver # webdriver를 이용해 해당 브라우저 열기\n",
    "from selenium.webdriver import ActionChains # 일련의 작업들을 연속적으로 진행할 수 있도록 도와줌\n",
    "from selenium.webdriver.common.keys import Keys # 키보드 입력을 할 수 있게 하기 위해\n",
    "from selenium.webdriver.common.by import By # html요소 탐색을 할 수 있게 하기 위해\n",
    "from selenium.webdriver.support.ui import WebDriverWait # 브라우저 응답을 기다릴 수 있게 하기 위해\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support import expected_conditions as EC # html 요소의 상태를 체크할 수 있게 하기 위해\n",
    "import pandas as pd\n",
    "\n",
    "# -- 처음 데이터 프레임 -- \n",
    "df = pd.read_csv('./NS_main_mid_sub1_cat.csv') \n",
    "\n",
    "def make_dict_by_mainCat(mainCat: str) -> dict:\n",
    "    \"\"\"\n",
    "    mainCat별 midCat과 sub1Cat 리스트로 나누어주는 \n",
    "    딕션어리를 만들어주는 함수입니다. \n",
    "    \"\"\"\n",
    "    main_mid_sub1_dict = {}\n",
    "    \n",
    "    # -- mainCat 기준으로 데이터 프레임 재생성 -- \n",
    "    df_by_main = df[df['mainCat_name'] == mainCat]\n",
    "    \n",
    "    # -- 데이터 프레임에서 unique한 midCat 구하기 --\n",
    "    midCat_list = list(df_by_main['midCat_name'].unique())\n",
    "    \n",
    "    # -- midCat에 따라 sub1Cat 뽑아와서 dictionary 형태로 만들기 --\n",
    "    mid_sub1_dict = defaultdict(list)\n",
    "    for midCat in midCat_list:\n",
    "        sub1_list = list(df_by_main[df_by_main['midCat_name'] == midCat]['sub1Cat_name'].unique()) # midCat에 따른 unique한 sub1 리스트\n",
    "        mid_sub1_dict[midCat] = sub1_list\n",
    "        \n",
    "    # -- mid_sub1_dict와 mainCat합쳐서 새로운 딕션어리 생성 --\n",
    "    main_mid_sub1_dict[mainCat] = dict(mid_sub1_dict)\n",
    "    \n",
    "    return main_mid_sub1_dict\n",
    "\n",
    "def bring_sub2Cat_id_from_url():\n",
    "    \"\"\"\n",
    "    url에서 sub2Cat_id만 가져오는 함수\n",
    "    \"\"\"\n",
    "    current_url = dr.current_url # 현재 주소에서 가져옴\n",
    "    cat_id = re.findall(r'\\/\\w+\\?', current_url)[0].lstrip('/').rstrip('?') \n",
    "    return cat_id\n",
    "\n",
    "def click_by_cat(cat):\n",
    "    \"\"\"\n",
    "    category별로 클릭해주는 함수\n",
    "    \"\"\"\n",
    "    el = dr.find_element_by_link_text(cat) \n",
    "    el.click()\n",
    "    ### 여기도 clickable 하게 만드는 코드 ###\n",
    "    return \n",
    "\n",
    "def bring_sub2Cat_Series(mainCat_dict: dict): # 대분류 딕션어리 하나 받았다고 가정하고\n",
    "    \n",
    "    sub2Cat_list = list()       #### 여기 바뀌어야되 ####\n",
    "    for mainCat, midCat_dict in mainCat_dict.items(): # mainCat & midCat list\n",
    "        for midCat, sub1Cat_list in midCat_dict.items(): # midCat & sub1Cat list\n",
    "            for sub1Cat in sub1Cat_list: # sub1Cat\n",
    "                \n",
    "                # -- 다음 sub2Cat 리스트 뽑아내기 --\n",
    "                url = 'https://search.shopping.naver.com/search/category/{}'.format(sub1Cat) # 처음부터 sub1Cat따라 url 들어오기\n",
    "                dr.get(url)\n",
    "                \n",
    "                # -- class 부분이 클릭이 가능해질 때까지 wait --\n",
    "                wait = WebDriverWait(driver, 8)\n",
    "                element = wait.until(EC.element_to_be_clickable((By.CLASS_NAME, 'filter_finder_list__4PE1C')))\n",
    "                \n",
    "                # -- beatifulsoup으로 sub2에 해당하는 리스트 가져오기 -- \n",
    "                soup = BeautifulSoup(dr.page_source, 'html.parser')\n",
    "                sub2_list = soup.select('.filter_finder_list__4PE1C .filter_text_over__iesoO')\n",
    "                sub2_list = [sub2.text for sub2 in sub2_list]\n",
    "                \n",
    "                if (sub2_list == sub1Cat_list) | (sub2_list == []):\n",
    "                    sub2Cat_list.append([sub1Cat, ' ', ' ']) # sub2가 없는 경우 빈칸과 '_' 조인\n",
    "                    \n",
    "                else:    \n",
    "                    for sub2Cat_name in sub2_list:\n",
    "                        # -- sub2Cat 클릭 --\n",
    "                        click_by_cat(sub2Cat)\n",
    "                        # -- 잠시 대기 --\n",
    "                        dr.implicitly_wait(3)\n",
    "                        # -- url에서 카테고리 아이디 가져오기 --\n",
    "                        sub2Cat_id = bring_sub2Cat_id_from_url()\n",
    "                        sub2Cat_list.append([sub1Cat, sub2Cat_id, sub2Cat_name]) # sub2 있는 경우 '_' 조인\n",
    "                        # -- 다시 뒤로 돌아가기 -- \n",
    "                        click_by_cat(sub1Cat)\n",
    "                        # -- 잠기 대기 --\n",
    "                        dr.implicitly_wait(3)\n",
    "                \n",
    "                dr.close() # 다음 카테고리로 넘어가기 위해 창 닫아주기\n",
    "                time.sleep(0.07)\n",
    "    sub2Cat_df = pd.DataFrame(sub2Cat_list, columns=['sub1Cat_name', 'sub2Cat_id', 'sub2Cat_name'])\n",
    "    return sub2Cat_df  # 기존 데이터 프레임에 merge시키기 위해 시리즈 데이터로 변경 \n",
    "    \n",
    "def merge_mainCat_sub2Cat_df(mainCat):\n",
    "    mainCat_dict = make_dict_by_mainCat(mainCat)\n",
    "    sub2Cat_df = bring_sub2Cat_Series(mainCat_dict)\n",
    "    df_by_main = df[df['mainCat_name'] == mainCat]\n",
    "    final_df = pd.merge(df_by_main, sub2Cat_df, left_on='sub1Cat_name', right_on='sub1Cat_name', how='left')\n",
    "    \n",
    "    return final_df # sub2Cat_name, sub2Cat_id까지 합쳐진 mainCat 별 데이터 프레임\n",
    "\n",
    "\n",
    "# 1. series 데이터 '_'로 분리시켜서 데이터 프레임으로 변경뒤\n",
    "# 2. 기존 데이터 프레임에 left merge 시켜주기 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b738b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 심지어 없는 부분에 대해서도 알아서 Nan 처리를 해줌 => 거의 다 왔다 조금만..!\n",
    "pd.merge(tmp_df, tmp_df2, left_on='sub1Cat_name', right_on = 0, how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc7acd8",
   "metadata": {},
   "source": [
    "# Open API 크롤링 -> 안씀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198fc11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "\n",
    "query = \"남성 모자\"\n",
    "query = urllib.parse.quote(query)\n",
    "\n",
    "display = \"10\"\n",
    "\n",
    "url = \"https://openapi.naver.com/v1/search/shop?query=\" + query + \"&display=\" + display\n",
    "\n",
    "request = urllib.request.Request(url)\n",
    "request.add_header('X-Naver-Client-Id', '5oXnmo8N6zm_ipiF9EEr')\n",
    "request.add_header('X-Naver-Client-Secret', '3y6lUnczAb')\n",
    "\n",
    "response = urllib.request.urlopen(request)\n",
    "print(response.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb58b37",
   "metadata": {},
   "source": [
    "# requests, json 크롤링 -> 최종"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e128a1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pycryptodome"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446db8d4",
   "metadata": {},
   "source": [
    "***catId를 넣어 크롤링 해오는 함수***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffca1cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from sbth import get_sbth\n",
    "\n",
    "def get_ns_prd_list(cat_id: str, page_num: int) ->list:\n",
    "    sbth = get_sbth()\n",
    "    ua = UserAgent(verify_ssl=False) \n",
    "    fake_ua = ua.random\n",
    "    headers = {\n",
    "        'user-agent' : fake_ua,\n",
    "        'cache-control' : 'no-cache, no-store, must-revalidate',\n",
    "        # -- sbth 없으면 정보 안가져옴 --\n",
    "        # -- update 필요할 수 있음 => 날짜 정보가 들어가 만료가 될 수 있음 -- \n",
    "        'sbth' : sbth,\n",
    "        # -- referer 없으면 정보 안가져옴 --\n",
    "        'referer' : 'https://search.shopping.naver.com/search/category',    \n",
    "    }\n",
    "\n",
    "    params = (\n",
    "        ('sort','rel'),\n",
    "        ('pagingIndex',page_num),\n",
    "        ('pagingSize','40'),\n",
    "        ('viewType','list'),\n",
    "        # -- 여기에 해당하는 catId는 엑셀파일에 있는 catId --\n",
    "        ('catId', cat_id),\n",
    "    )\n",
    "\n",
    "\n",
    "    url = 'https://search.shopping.naver.com/api/search/category'\n",
    "    res = requests.get(url, headers=headers, params=params)\n",
    "    if res.status_code == 200:\n",
    "        results = res.json()\n",
    "        results = results['shoppingResult']['products'] # products 키 값안에 상품 정보 들어있음. \n",
    "        \n",
    "        prd_list = list()\n",
    "        for result in results:\n",
    "            prd_id = result['id']\n",
    "            prd_name = result['productName']\n",
    "            cat1_id = result['category1Id']\n",
    "            cat1_name = result['category1Name']\n",
    "            cat2_id = result['category2Id']\n",
    "            cat2_name = result['category2Name']\n",
    "            cat3_id = result['category3Id']\n",
    "            cat3_name = result['category3Name']\n",
    "            # -- cat 4 name -- \n",
    "            \n",
    "            prd_image_url = result['imageUrl']\n",
    "            prd_low_price = result['lowPrice']\n",
    "            prd_list.append([prd_id, prd_name, cat1_id, cat1_name, cat2_id, \n",
    "                             cat2_name, cat3_id, cat3_name, prd_image_url, prd_low_price]) \n",
    "    else:\n",
    "        prd_list.append([None, None, None, None, None,\n",
    "                         None, None, None, None, None]) # 변수들 None 값으로 처리 \n",
    "        \n",
    "    return prd_list\n",
    "\n",
    "\n",
    "\n",
    "def get_prd_info(cat_id: str, page_count = 250):\n",
    "    \"\"\"\n",
    "    cat_id -> 크롤링하고자 하는 category id\n",
    "    page_count -> 크롤링하고자 하는 페이지 수 (250 pages by default)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    prd_list = list()\n",
    "    for page_num in range(1, page_count+1): # 250 페이지 가져오기 \n",
    "        prd_list += get_ns_prd_list(cat_id, page_num)\n",
    "        print('page %d done'%page_num)\n",
    "        \n",
    "        if page_num % 50 == 0:\n",
    "            print('-- paused for 5 sec in case not to be blocked --')\n",
    "            time.sleep(5)\n",
    "            \n",
    "    print('-- 데이터 개수 :  %d --'%(len(prd_list)))\n",
    "    \n",
    "    # -- prd_list에서 데이터 프레임 만들기 --\n",
    "    df = pd.DataFrame(prd_list, columns = ['prd_id', 'prd_name', 'cat1_id', 'cat1_name', 'cat2_id', \n",
    "                                           'cat2_name', 'cat3_id', 'cat3_name', 'prd_image_url', \n",
    "                                           'prd_low_price'])\n",
    "    end_time = time.time()\n",
    "    print(end_time - start_time)\n",
    "    return df\n",
    "\n",
    "\n",
    "get_prd_info('50000819', 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6d980",
   "metadata": {},
   "source": [
    "***catID url용 베이스 코드***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000bc2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from sbth import get_sbth\n",
    "\n",
    "sbth = get_sbth()\n",
    "ua = UserAgent(verify_ssl=False) \n",
    "fake_ua = ua.random\n",
    "headers = {\n",
    "    'user-agent' : fake_ua,\n",
    "    'cache-control' : 'no-cache, no-store, must-revalidate',\n",
    "    # -- sbth 없으면 정보 안가져옴 --\n",
    "    # -- update 필요할 수 있음 => 날짜 정보가 들어가 만료가 될 수 있음 -- \n",
    "    'sbth' : sbth,\n",
    "    # -- referer 없으면 정보 안가져옴 --\n",
    "    'referer' : 'https://search.shopping.naver.com/search/category',    \n",
    "}\n",
    "\n",
    "params = (\n",
    "    ('sort','rel'),\n",
    "    ('pagingIndex','2'),\n",
    "    ('pagingSize','40'),\n",
    "    ('viewType','list'),\n",
    "    # -- 여기에 해당하는 catId는 엑셀파일에 있는 catId --\n",
    "#     ('catId', cat_id),\n",
    ")\n",
    "\n",
    "\n",
    "# url = 'https://search.shopping.naver.com/api/search/category'\n",
    "# res = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "\n",
    "\n",
    "# url = 'https://search.shopping.naver.com/api/search/category/100005522'\n",
    "# url = 'https://search.shopping.naver.com/api/search/category/100005522?catId=50000832'\n",
    "url = 'https://search.shopping.naver.com/api/search/category/100000386?catId=50000425%2050000435%2050001487%2050001488'\n",
    "res = requests.get(url, headers=headers, params=params)\n",
    "data = res.json()\n",
    "data['shoppingResult']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3907ba4d",
   "metadata": {},
   "source": [
    "***검색 쿼리용 베이스 코드***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703383ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from sbth import get_sbth\n",
    "\n",
    "\n",
    "sbth = get_sbth() # sbth 가져오기\n",
    "ua = UserAgent(verify_ssl=False) # UserAgent 생성\n",
    "fake_ua = ua.random # UserAgent 생성\n",
    "\n",
    "headers = {\n",
    "    'user-agent' : fake_ua,\n",
    "    'cache-control' : 'no-cache, no-store, must-revalidate',\n",
    "    # -- sbth 없으면 정보 안가져옴 --\n",
    "    # -- 자동 sbth 생성 코드 적용 -- \n",
    "    'sbth' : sbth,\n",
    "    # -- referer 없으면 정보 안가져옴 --\n",
    "    'referer' : 'https://search.shopping.naver.com/search/all'\n",
    "}\n",
    "\n",
    "params = (\n",
    "    ('sort','rel'),\n",
    "#     ('origQuery', query),\n",
    "#     ('query', query),\n",
    "    ('productSet', 'total'), # 이게 뭐지?\n",
    "    ('pagingIndex', '2'),\n",
    "    ('pagingSize','40'),\n",
    "    ('viewType','list'),\n",
    ") \n",
    "\n",
    "url = 'https://search.shopping.naver.com/api/search/all?query=여성항공점퍼'\n",
    "res = requests.get(url, headers=headers, params=params)\n",
    "data = res.json()\n",
    "data['shoppingResult']['products']\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfaa37c",
   "metadata": {},
   "source": [
    "***크롤링에 사용될 수 있도록 url 전처리 해주는 함수***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420110d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def get_final_url_df():\n",
    "    \"\"\"\n",
    "    카테고리 매칭 작업했던 파일을 가져오고\n",
    "    최종 url에서 'api'단어를 붙이기 위해 준비해주는 함수. \n",
    "    \"\"\"\n",
    "    df = pd.read_csv('./카테고리 매칭 - 크롤링용_카테고리_수정본_0524.csv', header=1)\n",
    "    return df \n",
    "\n",
    "def add_api_into_url(url: str) -> str:\n",
    "    \"\"\"\n",
    "    url 안에 'api'단어를 붙여주는 함수.\n",
    "    'api'단어가 있어야 크롤링 가능. \n",
    "    \"\"\"\n",
    "    new_url = None\n",
    "    try:\n",
    "        new_url = re.sub('(.com\\/search)', '.com/api/search', url)\n",
    "    except:\n",
    "        new_url = 'no_url'\n",
    "    return new_url\n",
    "\n",
    "def return_url_list():\n",
    "    \"\"\"\n",
    "    크롤링이 가능한 url을 반환해주는 함수.\n",
    "    \"\"\"\n",
    "    df = get_final_url_df()\n",
    "    df['url_for_crawling'] = df['temp URL'].map(lambda x : add_api_into_url(x))\n",
    "    url_list = list(df['url_for_crawling'])\n",
    "    url_list = [url for url in url_list if url != 'no_url']\n",
    "    \n",
    "    # -- 리스트 txt 파일로 만들어주기 -- \n",
    "    with open('ns_urls.txt', 'w+') as file:\n",
    "        file.write(','.join(url_list))\n",
    "        \n",
    "    return \n",
    "\n",
    "from ns_make_urls_for_crawling import return_url_list\n",
    "return_url_list()\n",
    "\n",
    "# list로 다시 불러오기\n",
    "with open('./ns_urls.txt', 'r') as f:\n",
    "    naver_urls = f.read()\n",
    "naver_url_list = naver_urls.split(',')\n",
    "\n",
    "print(len(naver_url_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a1d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ns_make_urls_for_crawling import return_url_list\n",
    "return_url_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a0033e",
   "metadata": {},
   "source": [
    "***url을 넣어서 크롤링을 해오는 최종 함수***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6471fd07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>image_url</th>\n",
       "      <th>low_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39272398567</td>\n",
       "      <td>절개 스티치 통굽 스퀘어 뮬 블로퍼힐 청키뮬</td>\n",
       "      <td>https://shopping-phinf.pstatic.net/main_392723...</td>\n",
       "      <td>32100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40155439133</td>\n",
       "      <td>Josef Seibel 힐 뮬 다크</td>\n",
       "      <td>https://shopping-phinf.pstatic.net/main_401554...</td>\n",
       "      <td>144650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40128517124</td>\n",
       "      <td>베르사체 그린 메두사 비기 뮬</td>\n",
       "      <td>https://shopping-phinf.pstatic.net/main_401285...</td>\n",
       "      <td>1178000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40128499512</td>\n",
       "      <td>LAURA VITA 힐 뮬 rot kombi</td>\n",
       "      <td>https://shopping-phinf.pstatic.net/main_401284...</td>\n",
       "      <td>138540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40040927618</td>\n",
       "      <td>배색 스퀘어 쪼리힐 스트랩 뮬슬리퍼</td>\n",
       "      <td>https://shopping-phinf.pstatic.net/main_400409...</td>\n",
       "      <td>30980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>795</th>\n",
       "      <td>40026787156</td>\n",
       "      <td>발이편한 사무실슬리퍼 통굽 세련된 블로퍼 여성신발</td>\n",
       "      <td>https://shopping-phinf.pstatic.net/main_400267...</td>\n",
       "      <td>32030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>40154791314</td>\n",
       "      <td>BV CURVE LEATHER MULES 618757</td>\n",
       "      <td>https://shopping-phinf.pstatic.net/main_401547...</td>\n",
       "      <td>1249810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>40154792189</td>\n",
       "      <td>해외배송 앰부쉬 레더 플랫폼 뮬 75I-Y4Z010 M0060099840</td>\n",
       "      <td>https://shopping-phinf.pstatic.net/main_401547...</td>\n",
       "      <td>744010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>798</th>\n",
       "      <td>40154796882</td>\n",
       "      <td>Moon 부츠 울 스웨이드 뮬 w 레이스 여성 6012900</td>\n",
       "      <td>https://shopping-phinf.pstatic.net/main_401547...</td>\n",
       "      <td>451572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>799</th>\n",
       "      <td>40154783237</td>\n",
       "      <td>해외배송 아쿠아주라 여성 뮬 17491056PU 3361746</td>\n",
       "      <td>https://shopping-phinf.pstatic.net/main_401547...</td>\n",
       "      <td>551500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>800 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                      name  \\\n",
       "0    39272398567                  절개 스티치 통굽 스퀘어 뮬 블로퍼힐 청키뮬   \n",
       "1    40155439133                       Josef Seibel 힐 뮬 다크   \n",
       "2    40128517124                          베르사체 그린 메두사 비기 뮬   \n",
       "3    40128499512                  LAURA VITA 힐 뮬 rot kombi   \n",
       "4    40040927618                       배색 스퀘어 쪼리힐 스트랩 뮬슬리퍼   \n",
       "..           ...                                       ...   \n",
       "795  40026787156               발이편한 사무실슬리퍼 통굽 세련된 블로퍼 여성신발   \n",
       "796  40154791314             BV CURVE LEATHER MULES 618757   \n",
       "797  40154792189  해외배송 앰부쉬 레더 플랫폼 뮬 75I-Y4Z010 M0060099840   \n",
       "798  40154796882         Moon 부츠 울 스웨이드 뮬 w 레이스 여성 6012900   \n",
       "799  40154783237        해외배송 아쿠아주라 여성 뮬 17491056PU 3361746   \n",
       "\n",
       "                                             image_url low_price  \n",
       "0    https://shopping-phinf.pstatic.net/main_392723...     32100  \n",
       "1    https://shopping-phinf.pstatic.net/main_401554...    144650  \n",
       "2    https://shopping-phinf.pstatic.net/main_401285...   1178000  \n",
       "3    https://shopping-phinf.pstatic.net/main_401284...    138540  \n",
       "4    https://shopping-phinf.pstatic.net/main_400409...     30980  \n",
       "..                                                 ...       ...  \n",
       "795  https://shopping-phinf.pstatic.net/main_400267...     32030  \n",
       "796  https://shopping-phinf.pstatic.net/main_401547...   1249810  \n",
       "797  https://shopping-phinf.pstatic.net/main_401547...    744010  \n",
       "798  https://shopping-phinf.pstatic.net/main_401547...    451572  \n",
       "799  https://shopping-phinf.pstatic.net/main_401547...    551500  \n",
       "\n",
       "[800 rows x 4 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- base code -- \n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from sbth import get_sbth\n",
    "\n",
    "prd_list = list()\n",
    "for idx in range(200, 220):\n",
    "    sbth = get_sbth()\n",
    "    ua = UserAgent(verify_ssl=False) \n",
    "    fake_ua = ua.random\n",
    "    headers = {\n",
    "        'user-agent' : fake_ua,\n",
    "        'cache-control' : 'no-cache, no-store, must-revalidate',\n",
    "        'sbth' : sbth,\n",
    "        'referer' : 'https://search.shopping.naver.com/search/category',    \n",
    "    }\n",
    "\n",
    "    params = (\n",
    "        ('sort','rel'),\n",
    "        ('pagingIndex', idx),\n",
    "        ('productSet', 'total'), \n",
    "        ('pagingSize','40'),\n",
    "        ('viewType','list'),\n",
    "        ('iq', ''),\n",
    "        ('spec', '')\n",
    "    )\n",
    "\n",
    "\n",
    "    url = 'https://search.shopping.naver.com/api/search/category/100000076?catId=50003847' \n",
    "    res = requests.get(url, headers=headers, params=params)\n",
    "    results = res.json()\n",
    "\n",
    "    try:\n",
    "        results = results['shoppingResult']['products']\n",
    "        for result in results:\n",
    "            prd_id = result['id'] # id\n",
    "            prd_name = result['productName'] # name\n",
    "            prd_image_url = result['imageUrl'] # image_url\n",
    "            prd_low_price = result['lowPrice'] # low_price\n",
    "\n",
    "            prd_list.append([str(prd_id), prd_name, prd_image_url, str(prd_low_price)]) \n",
    "    except:\n",
    "        pass\n",
    "\n",
    "pd.DataFrame(prd_list, columns=['id', 'name', 'image_url', 'low_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21eec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- cat_id_list_json + 최종 카테고리 매칭 엑셀 파일 합치는 함수 --\n",
    "def two_files_to_one_df():\n",
    "    \n",
    "    # -- 카테고리 매칭 - 최종 카테고리 데이터 프레임으로 --\n",
    "    # -- 필요할 것 같은 컬럼만 가져옴 => 변경 가능 --\n",
    "    df1 = pd.read_csv('./카테고리 매칭 - 크롤링용_카테고리_수정본_0524.csv', header=1)\n",
    "    df1 = df1[['cat1', 'cat2', 'cat3', 'cat', '최종 URL', 'iq', 'spec']].copy()\n",
    "    \n",
    "    # -- cat_id_list.json 파일 데이터 프레임으로 --\n",
    "    with open('./cate_id_list.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "    df2 = pd.DataFrame(list(data.values()), columns = ['cat1', 'cat2', 'cat3'])\n",
    "    df2['cat_id'] = list(data.keys())\n",
    "\n",
    "    # -- cat1, cat2, cat3 기준으로 merge --\n",
    "    merged_df = pd.merge(df1, df2, how='left', on=['cat1', 'cat2', 'cat3'])\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "merged_df = two_files_to_one_df()\n",
    "merged_df.to_csv('./json_and_excel_files_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8baefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- main 크롤링 함수 -- \n",
    "import requests\n",
    "from fake_useragent import UserAgent\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def open_and_preprocess_df():\n",
    "    df = pd.read_csv('/content/drive/MyDrive/json_and_excel_files_merged.csv')\n",
    "    df = df.fillna('')\n",
    "    df['url'] = df['최종 URL'] # url 컬럼 이름 바꾸기\n",
    "    del df['최종 URL']\n",
    "\n",
    "    # -- cat3 기준 기타 빼야 됨 --\n",
    "    # -- 그리고 reset index --\n",
    "    index_list = list(df['cat3'][df['cat3'].str.contains('기타')].index)\n",
    "    df = df.drop(index=index_list).copy()\n",
    "    df = df.reset_index(drop=True).copy()\n",
    "    \n",
    "    # -- url 부분에 api 단어 추가 -- \n",
    "    df['url'] = df['url'].map(lambda x : re.sub('(.com\\/search)', '.com/api/search', x))\n",
    "    # -- url 중간에 빈칸이 들어가 있는 경우가 있었음 빈칸 제거 --\n",
    "    df['url'] = df['url'].map(lambda x : re.sub('\\s+', '', x))\n",
    "\n",
    "    # -- cat_id 타입 int로 바꾸기 --\n",
    "    df['cat_id'] = df['cat_id'].astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def base_crawling_code(url:str, iq:str, spec:str, cat_id:int, page_num:int):\n",
    "    \"\"\"\n",
    "    url, iq, spec 세개를 인자로 받아 크롤링을 하는 함수. \n",
    "    \"\"\"\n",
    "    sbth = get_sbth()\n",
    "    ua = UserAgent(verify_ssl=False) \n",
    "    fake_ua = ua.random\n",
    "    headers = {\n",
    "        'user-agent' : fake_ua,\n",
    "        'cache-control' : 'no-cache, no-store, must-revalidate',\n",
    "        'sbth' : sbth,\n",
    "        'referer' : 'https://search.shopping.naver.com/search/category',    \n",
    "    }\n",
    "\n",
    "    params = (\n",
    "        ('sort','rel'),\n",
    "        ('pagingIndex', str(page_num)),\n",
    "        ('productSet', 'total'), \n",
    "        ('pagingSize','40'),\n",
    "        ('viewType','list'),\n",
    "        ('iq', iq),\n",
    "        ('spec', spec)\n",
    "    )\n",
    "        \n",
    "    prd_list = list()\n",
    "    res = requests.get(url, headers=headers, params=params)\n",
    "    if (res.status_code == 200):\n",
    "        results = res.json()\n",
    "\n",
    "        try:\n",
    "            results = results['shoppingResult']['products'] # products 키 값안에 상품 정보 들어있음. \n",
    "            \n",
    "            for result in results:\n",
    "                prd_id = result['id'] # id\n",
    "                prd_name = result['productName'] # name\n",
    "                prd_image_url = result['imageUrl'] # image_url\n",
    "                prd_low_price = result['lowPrice'] # low_price\n",
    "\n",
    "                prd_list.append([str(prd_id), prd_name, str(cat_id), prd_image_url, str(prd_low_price)])\n",
    "        except:\n",
    "            pass \n",
    "            \n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return prd_list\n",
    "\n",
    "def get_NaverPrd(url:str, iq:str, spec:str, cat_id:int, page_count = 400):\n",
    "    \"\"\"\n",
    "    cat_id -> 크롤링하고자 하는 category id\n",
    "    page_count -> 크롤링하고자 하는 페이지 수 (300 pages by default)\n",
    "    \"\"\"\n",
    "    prd_list = list()\n",
    "    for page_num in range(1, page_count+1): # 300 페이지 가져오기 \n",
    "        prd_list += base_crawling_code(url, iq, spec, int(cat_id), page_num)\n",
    "        \n",
    "        if page_num % 50 == 0:\n",
    "            time.sleep(5)\n",
    "            \n",
    "    print(f'-- category id {int(cat_id)} done --')\n",
    "    print(f'-- 데이터 개수 : {len(prd_list)} --')\n",
    "    \n",
    "    # -- prd_list에서 데이터 프레임 만들기 --\n",
    "    df = pd.DataFrame(prd_list, columns = ['id', 'name', 'cat_id', 'image_url', 'low_price'])\n",
    "    \n",
    "    # -- csv 파일로 내보내기 --\n",
    "    # -- why? 혹시나 막힐 수도 있으니 미리미리 데이터 프레임으로 만들어 놓자 -- \n",
    "    df.to_csv(f'/content/drive/MyDrive/cat_id_{int(cat_id)}_DF.csv', index=False)\n",
    "    \n",
    "    return\n",
    "\n",
    "def run_all_process():\n",
    "    df = open_and_preprocess_df()\n",
    "    \n",
    "    for idx in range(44, 96):  # 13부터 막혀서 13부터 다시 시작. \n",
    "        url = df.iloc[idx]['url']\n",
    "        iq = df.iloc[idx]['iq']\n",
    "        spec = df.iloc[idx]['spec']\n",
    "        cat_id = df.iloc[idx]['cat_id']\n",
    "        \n",
    "        get_NaverPrd(url, iq, spec, cat_id, page_count=300) # page_count는 default로 400 페이지 그러나 여기서는 300\n",
    "        \n",
    "        # -- 블락을 대비하여 좀 길게 time.sleep() --\n",
    "        print(f'-- {idx} 번째 카테고리 done --')\n",
    "        print('\\n-- paused for 30 sec not to be blocked after crawling one category --\\n')\n",
    "        time.sleep(10)\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "435c6a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def open_and_preprocess_df():\n",
    "    df = pd.read_csv('./json_and_excel_files_merged.csv')\n",
    "    df = df.fillna('')\n",
    "    df['url'] = df['최종 URL'] # url 컬럼 이름 바꾸기\n",
    "    del df['최종 URL']\n",
    "\n",
    "    # -- cat3 기준 기타 빼야 됨 --\n",
    "    # -- 그리고 reset index --\n",
    "    index_list = list(df['cat3'][df['cat3'].str.contains('기타')].index)\n",
    "    df = df.drop(index=index_list).copy()\n",
    "    df = df.reset_index(drop=True).copy()\n",
    "    \n",
    "    # -- url 부분에 api 단어 추가 -- \n",
    "    df['url'] = df['url'].map(lambda x : re.sub('(.com\\/search)', '.com/api/search', x))\n",
    "\n",
    "    # -- cat_id 타입 int로 바꾸기 --\n",
    "    df['cat_id'] = df['cat_id'].astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = open_and_preprocess_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "435cb11c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cat1                                                   남성의류\n",
       "cat2                                                     바지\n",
       "cat3                                                    면바지\n",
       "cat                                                        \n",
       "iq                                                         \n",
       "spec                                                       \n",
       "cat_id                                            320120100\n",
       "url       https://search.shopping.naver.com/api/search/a...\n",
       "Name: 71, dtype: object"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f7b00b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('./json_and_excel_files_merged.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7356b4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://search.shopping.naver.com/api/search/category/100000076?catId=50003847'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_url = 'https://search.shopping.naver.com/api/search/category/100000076?  catId=50003847  '\n",
    "re.sub('\\s+', '', tmp_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
